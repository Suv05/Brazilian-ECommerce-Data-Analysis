{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94edd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß SOLUTION 1: Check if Spark session exists, create if needed\n",
    "def get_spark_session():\n",
    "    try:\n",
    "        # Try to get existing active session\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark is None:\n",
    "            # Create new session if none exists\n",
    "            spark = SparkSession.builder \\\n",
    "                .appName(\"Module-5\") \\\n",
    "                .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\") \\\n",
    "                .getOrCreate()\n",
    "        return spark\n",
    "    except:\n",
    "        # If anything goes wrong, create fresh session\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"Module-5\") \\\n",
    "            .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\") \\\n",
    "            .getOrCreate()\n",
    "        return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or create Spark session\n",
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7efef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Spark Session Status:\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Active: {not spark.sparkContext._jsc.sc().isStopped()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d475f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base GCS bucket path\n",
    "gcs_bucket_path = \"gs://retail-order-data-bucket/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each Parquet file into a DataFrame\n",
    "# Customers DataFrame\n",
    "customers_df = spark.read.parquet(f\"{gcs_bucket_path}customers_clean_df.parquet\")\n",
    "# Geolocation DataFrame\n",
    "geolocation_df = spark.read.parquet(f\"{gcs_bucket_path}geolocation_clean_df.parquet/\")\n",
    "# Order Items DataFrame\n",
    "order_items_df = spark.read.parquet(f\"{gcs_bucket_path}order_items_clean_df.parquet/\")\n",
    "# Order Payments DataFrame\n",
    "order_payments_df = spark.read.parquet(f\"{gcs_bucket_path}order_payments_clean_df.parquet/\")\n",
    "# Orders DataFrame\n",
    "orders_df = spark.read.parquet(f\"{gcs_bucket_path}orders_clean_df.parquet/\")\n",
    "# Sellers DataFrame\n",
    "sellers_df = spark.read.parquet(f\"{gcs_bucket_path}sellers_clean_df.parquet/\")\n",
    "# Order Reviews DataFrame\n",
    "order_reviews_df = spark.read.parquet(f\"{gcs_bucket_path}orders_reviews_clean_df.parquet/\")\n",
    "# Products DataFrame\n",
    "products_df = spark.read.parquet(f\"{gcs_bucket_path}products_clean_df.parquet/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11874d9f",
   "metadata": {},
   "source": [
    "### Data Serving Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- 1. Pre-process Geolocation Data ---\n",
    "# Average lat/lng for each zip code prefix to get a unique coordinate set.\n",
    "avg_geolocation_df = geolocation_df.groupBy(\"geolocation_zip_code_prefix\").agg(\n",
    "    avg(\"geolocation_lat\").alias(\"avg_lat\"),\n",
    "    avg(\"geolocation_lng\").alias(\"avg_lng\")\n",
    ")\n",
    "\n",
    "# --- 2. Join Core Order, Customer, Payment, and Review Data using INNER JOIN ---\n",
    "# Only orders with matches in all these core tables will be kept.\n",
    "combined_df = orders_df.alias(\"o\") \\\n",
    "    .join(customers_df.alias(\"c\"), col(\"o.customer_id\") == col(\"c.customer_id\"), \"inner\") \\\n",
    "    .join(order_payments_df.alias(\"op\"), col(\"o.order_id\") == col(\"op.order_id\"), \"inner\") \\\n",
    "    .join(order_reviews_df.alias(\"orv\"), col(\"o.order_id\") == col(\"orv.order_id\"), \"inner\")\n",
    "\n",
    "# --- 3. Join Order Items, Products, and Sellers Data using INNER JOIN ---\n",
    "# Only records with matches in these will be kept.\n",
    "combined_df = combined_df \\\n",
    "    .join(order_items_df.alias(\"oi\"), col(\"o.order_id\") == col(\"oi.order_id\"), \"inner\") \\\n",
    "    .join(products_df.alias(\"p\"), col(\"oi.product_id\") == col(\"p.product_id\"), \"inner\") \\\n",
    "    .join(sellers_df.alias(\"s\"), col(\"oi.seller_id\") == col(\"s.seller_id\"), \"inner\")\n",
    "\n",
    "# --- 4. Join Geolocation Data for Customers using INNER JOIN ---\n",
    "# Only records where customer zip code has a geolocation match will be kept.\n",
    "combined_df = combined_df.join(\n",
    "    avg_geolocation_df.alias(\"geo_cust\"),\n",
    "    col(\"c.customer_zip_code_prefix\") == col(\"geo_cust.geolocation_zip_code_prefix\"),\n",
    "    \"inner\"\n",
    ").withColumnRenamed(\"avg_lat\", \"customer_lat\") \\\n",
    " .withColumnRenamed(\"avg_lng\", \"customer_lng\")\n",
    "\n",
    "# --- 5. Join Geolocation Data for Sellers using INNER JOIN ---\n",
    "# Only records where seller zip code has a geolocation match will be kept.\n",
    "combined_df = combined_df.join(\n",
    "    avg_geolocation_df.alias(\"geo_seller\"),\n",
    "    col(\"s.seller_zip_code_prefix\") == col(\"geo_seller.geolocation_zip_code_prefix\"),\n",
    "    \"inner\"\n",
    ").withColumnRenamed(\"avg_lat\", \"seller_lat\") \\\n",
    " .withColumnRenamed(\"avg_lng\", \"seller_lng\")\n",
    "\n",
    "# --- 6. Select and Rename Columns for Final Output ---\n",
    "# This step is crucial to avoid duplicate column names and make the schema clean\n",
    "final_combined_df = combined_df.select(\n",
    "    # Order Details\n",
    "    col(\"o.order_id\"),\n",
    "    col(\"o.order_status\"),\n",
    "    col(\"o.order_purchase_timestamp\"),\n",
    "    col(\"o.order_approved_at\"),\n",
    "    col(\"o.order_delivered_carrier_date\"),\n",
    "    col(\"o.order_delivered_customer_date\"),\n",
    "    col(\"o.order_estimated_delivery_date\"),\n",
    "\n",
    "    # Customer Details\n",
    "    col(\"c.customer_id\"),\n",
    "    col(\"c.customer_unique_id\"),\n",
    "    col(\"c.customer_zip_code_prefix\"),\n",
    "    col(\"c.customer_city\"),\n",
    "    col(\"c.customer_state\"),\n",
    "    col(\"customer_lat\"), # from geo_cust join\n",
    "    col(\"customer_lng\"), # from geo_cust join\n",
    "\n",
    "    # Order Item Details\n",
    "    col(\"oi.order_item_id\"),\n",
    "    col(\"oi.product_id\"),\n",
    "    col(\"oi.price\").alias(\"item_price\"), # Renamed to avoid conflict with payment_value\n",
    "    col(\"oi.freight_value\").alias(\"item_freight_value\"),\n",
    "    col(\"oi.total_value\").alias(\"item_total_value\"), # total_value from order_items\n",
    "    col(\"oi.shipping_limit_date\"),\n",
    "    col(\"oi.shipping_date\"),\n",
    "    col(\"oi.shipping_year\"),\n",
    "    col(\"oi.shipping_month\"),\n",
    "\n",
    "    # Product Details\n",
    "    col(\"p.product_category_name\"),\n",
    "    col(\"p.product_name_lenght\"),\n",
    "    col(\"p.product_description_lenght\"),\n",
    "    col(\"p.product_photos_qty\"),\n",
    "    col(\"p.product_weight_g\"),\n",
    "    col(\"p.product_length_cm\"),\n",
    "    col(\"p.product_height_cm\"),\n",
    "    col(\"p.product_width_cm\"),\n",
    "    col(\"p.product_weight_kg\"),\n",
    "    col(\"p.product_category_clean\"),\n",
    "\n",
    "    # Seller Details\n",
    "    col(\"s.seller_id\"),\n",
    "    col(\"s.seller_zip_code_prefix\"),\n",
    "    col(\"s.seller_city\"),\n",
    "    col(\"s.seller_state\"),\n",
    "    col(\"seller_lat\"), # from geo_seller join\n",
    "    col(\"seller_lng\"), # from geo_seller join\n",
    "\n",
    "    # Payment Details\n",
    "    col(\"op.payment_sequential\"),\n",
    "    col(\"op.payment_type\"),\n",
    "    col(\"op.payment_installments\"),\n",
    "    col(\"op.payment_value\").alias(\"order_payment_value\"), # Renamed for clarity\n",
    "    col(\"op.installment_value\"),\n",
    "\n",
    "    # Review Details\n",
    "    col(\"orv.review_id\"),\n",
    "    col(\"orv.review_score\"),\n",
    "    col(\"orv.review_comment_title\"),\n",
    "    col(\"orv.review_comment_message\"),\n",
    "    col(\"orv.review_creation_date\"),\n",
    "    col(\"orv.review_answer_timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998419e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your temp GCS bucket path\n",
    "temp_gcs_bucket = \"temp-buck-111\"  # just the bucket name, without gs://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9143e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    final_combined_df.write \\\n",
    "        .format(\"bigquery\") \\\n",
    "        .option(\"temporaryGcsBucket\", temp_gcs_bucket) \\\n",
    "        .option(\"table\", bigquery_table) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "    print(f\"‚úÖ Successfully wrote combined data to BigQuery table: {bigquery_table}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error writing to BigQuery: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
